{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First glance and etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "import math\n",
    "from PIL import Image\n",
    "import dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WHT = 160\n",
    "IMG_HHT = 160\n",
    "ETA = 1e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weigth_initialization(conv_weigths, init):\n",
    "    if (init == 'xavier_normal'):\n",
    "        nn.init.xavier_normal_(conv_weigths)\n",
    "    if (init == 'xavier_uniform'):\n",
    "        nn.init.xavier_uniform_(conv_weigths)\n",
    "    return\n",
    "        \n",
    "\n",
    "def conv(input_channels, \n",
    "         output_channels,\n",
    "         kernel_size, stride, \n",
    "         padding, weigth_init = 'xavier_normal', \n",
    "         batch_norm = False,\n",
    "         activation=nn.ReLU()):\n",
    "    a = []\n",
    "    a.append(nn.Conv2d(input_channels, output_channels, kernel_size=kernel_size, stride=stride, padding=padding))\n",
    "    weigth_initialization(a[-1].weight, weigth_init)\n",
    "    if activation is not None:\n",
    "        a.append(activation)\n",
    "    if batch_norm:\n",
    "        a.append(nn.BatchNorm2d(output_channels))\n",
    "    return nn.Sequential(*a)\n",
    "\n",
    "def deconv(input_channels, \n",
    "         output_channels,\n",
    "         kernel_size, stride, \n",
    "         padding, weigth_init = 'xavier_normal', \n",
    "         batch_norm = False,\n",
    "         activation=nn.ReLU()):\n",
    "    a = []\n",
    "    a.append(nn.ConvTranspose2d(input_channels, output_channels, kernel_size=kernel_size, stride=stride, padding=padding))\n",
    "    weigth_initialization(a[-1].weight, weigth_init)\n",
    "    if activation is not None:\n",
    "        a.append(activation)\n",
    "    if batch_norm:\n",
    "        a.append(nn.BatchNorm2d(output_channels))\n",
    "    return nn.Sequential(*a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, input_channels,\n",
    "                output_channels = None, \n",
    "                kernel_size = 3, stride = 1, \n",
    "                padding = None, weight_init = 'xavier_normal', \n",
    "                batch_norm = False,\n",
    "                activation=nn.ReLU()):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.activation = activation\n",
    "        if output_channels is None:\n",
    "            output_channels = input_channels // stride\n",
    "        if stride == 1:\n",
    "            self.shortcut = nn.Sequential()\n",
    "        else:\n",
    "            self.shortcut = nn.conv(input_channels, output_channels, 1, stride, 0, None, False, None)\n",
    "            \n",
    "        a = []\n",
    "        a.append( conv( input_channels , input_channels  , kernel_size , 1 , padding if padding is not None else (kernel_size - 1)//2 , weight_init ,  False, activation))\n",
    "        a.append( conv( input_channels , output_channels , kernel_size , 1 , padding if padding is not None else (kernel_size - 1)//2 , None , False, None))\n",
    "        self.model = nn.Sequential(*a)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.activation(self.model(x) + self.shortcut(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formula to calculate padding (for me)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output_size: $$O = \\Big[\\frac{W−F+2P}{S}\\Big]+1$$ where W - input_size, F - filter size, P - padding, S - stride $$$$\n",
    "Therefore, $$P = \\frac{S \\cdot (O - 1) + F - W}{2} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_conv_outsize(input_size, filter_size, stride, pad):\n",
    "    return math.floor((input_size - filter_size + 2 * pad) / stride) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_half_up(n):\n",
    "    return math.floor(n + 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_conv_pad(input_size, output_size, filter_size, stride):\n",
    "    return round_half_up((stride * (output_size - 1) + filter_size - input_size) / 2)\n",
    "\n",
    "def calc_deconv_pad(input_size, output_size, filter_size, stride):\n",
    "    return round_half_up((stride * (input_size - 1) + filter_size - output_size) / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_padding(size, kernel_size, stride, dilation):\n",
    "    return ((size - 1) * (stride - 1) + dilation * (kernel_size - 1)) // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        ##ENCODER##\n",
    "\n",
    "        \n",
    "        f = np.array([64, 64, 128, 256, 512])\n",
    "        \n",
    "        f = f.astype(int)\n",
    "\n",
    "        \n",
    "        batch_norm = True\n",
    "        self.conv0 = nn.Sequential(conv(9, f[0], 7, 1, calc_conv_pad(IMG_WHT, IMG_WHT, 7, 1), \"xavier_normal\", batch_norm, nn.ReLU(ETA)), # 128 x 128\n",
    "                                   ResidualBlock(f[0], activation = nn.ReLU(ETA))) \n",
    "        self.conv1 = nn.Sequential(conv(f[0], f[1], 5, 2, calc_conv_pad(IMG_WHT, IMG_WHT / 2, 5, 2), \"xavier_normal\",  batch_norm, nn.ReLU(ETA)), # 64 x 64\n",
    "                                   ResidualBlock(f[1], activation = nn.ReLU(ETA)))\n",
    "        self.conv2 = nn.Sequential(conv(f[1], f[2], 3, 2, calc_conv_pad(IMG_WHT / 2, IMG_WHT / 4, 3, 2), \"xavier_normal\", batch_norm, nn.ReLU(ETA)),  # 32 x 32\n",
    "                                   ResidualBlock(f[2], activation = nn.ReLU(ETA))) \n",
    "        self.conv3 = nn.Sequential(conv(f[2], f[3], 3, 2, calc_conv_pad(IMG_WHT / 4, IMG_WHT / 8, 3, 2), \"xavier_normal\",  batch_norm, nn.ReLU(ETA)), # 16 x 16\n",
    "                                   ResidualBlock(f[3], activation = nn.ReLU(ETA))) \n",
    "        self.conv4 = nn.Sequential(conv(f[3], f[4], 3, 2, calc_conv_pad(IMG_WHT / 8, IMG_WHT / 16, 3, 2), \"xavier_normal\", batch_norm, nn.ReLU(ETA)), # 8 x 8\n",
    "                                   ResidualBlock(f[4], activation = nn.ReLU(ETA))) \n",
    "        self.fc1 = nn.Linear(f[4] * 10 * 10, f[4])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxout = nn.MaxPool1d(2 )\n",
    "        ##DECODER##\n",
    "        \n",
    "        self.fc2 = nn.Linear(f[3], f[1] * 10 * 10)\n",
    "        \n",
    "        #first path - 3 deconvs\n",
    "        \n",
    "        \n",
    "        f = np.array([64, 32, 16, 8])\n",
    "        f = f.astype(int)\n",
    "\n",
    "        \n",
    "        self.dc0_1 = deconv(f[0], f[1], 4, 4, calc_deconv_pad(IMG_WHT / 16, IMG_WHT / 4, 4, 4), \"xavier_normal\", batch_norm, activation=nn.ReLU(ETA)) # 32 x 32\n",
    "        self.dc0_2 = deconv(f[1], f[2], 2, 2, calc_deconv_pad(IMG_WHT / 4, IMG_WHT / 2, 2, 2), \"xavier_normal\", batch_norm, activation=nn.ReLU(ETA)) # 64 x 64\n",
    "        self.dc0_3 = deconv(f[2], f[3], 2, 2, calc_deconv_pad(IMG_WHT / 2, IMG_WHT, 2, 2), \"xavier_normal\", batch_norm, activation=nn.ReLU(ETA)) # 128 x 128\n",
    "        \n",
    "        #u-net path - 4 deconvs\n",
    "\n",
    "        \n",
    "        f = np.array([512, 256, 128, 64, 32, 16, 8])  \n",
    "        f = f.astype(int)\n",
    "\n",
    "        \n",
    "        self.dc1 = nn.Sequential(deconv(f[0] + f[3], f[0], 2, 2, calc_deconv_pad(IMG_WHT / 16, IMG_WHT / 8, 2, 2),\"xavier_normal\", batch_norm, activation=nn.ReLU(ETA)), # 16 x 16\n",
    "                                 ResidualBlock(f[0], activation = nn.ReLU(ETA)),\n",
    "                                 ResidualBlock(f[0], activation = nn.ReLU(ETA)))\n",
    "        self.dc2 = nn.Sequential(deconv(f[0] + f[1], f[1], 2, 2, calc_deconv_pad(IMG_WHT / 8, IMG_WHT / 4, 2, 2),\"xavier_normal\", batch_norm, activation=nn.ReLU(ETA)), # 32 x 32\n",
    "                                 ResidualBlock(f[1], activation = nn.ReLU(ETA)),\n",
    "                                 ResidualBlock(f[1], activation = nn.ReLU(ETA)))\n",
    "        self.dc3 = nn.Sequential(deconv(f[2] + f[1] + 3 + f[4], f[2], 2, 2, calc_deconv_pad(IMG_WHT / 4, IMG_WHT / 2, 2, 2), \"xavier_normal\", batch_norm, activation=nn.ReLU(ETA)), # 64 x 64\n",
    "                                 ResidualBlock(f[2], activation = nn.ReLU(ETA)),\n",
    "                                 ResidualBlock(f[2], activation = nn.ReLU(ETA)))\n",
    "        self.dc4 = nn.Sequential(deconv(f[2] + f[3] + 3 + f[5], f[3], 2, 2, calc_deconv_pad(IMG_WHT / 2, IMG_WHT, 2, 2), \"xavier_normal\", batch_norm, activation=nn.ReLU(ETA)), # 128 x 128\n",
    "                                 ResidualBlock(f[3], activation = nn.ReLU(ETA)),\n",
    "                                 ResidualBlock(f[3], activation = nn.ReLU(ETA)))\n",
    "\n",
    "        #final convs\n",
    "        \n",
    "        self.conv5 = conv(f[1], 3, 3, 1, calc_conv_pad(IMG_WHT / 4, IMG_WHT / 4, 3, 1), \"xavier_normal\", batch_norm, nn.ReLU(ETA)) # 32 x 32\n",
    "        self.conv6 = conv(f[2], 3, 3, 1, calc_conv_pad(IMG_WHT / 2, IMG_WHT / 2, 3, 1), \"xavier_normal\", batch_norm, nn.ReLU(ETA)) # 64 x 64\n",
    "        self.conv7 = conv(f[3] + f[3] + 3 + f[6], f[3], 5, 1, calc_conv_pad(IMG_WHT, IMG_WHT, 5, 1), \"xavier_normal\", batch_norm, nn.ReLU(ETA)) # 128 x 128\n",
    "        self.conv8 = conv(f[3], f[4], 3, 1, calc_conv_pad(IMG_WHT, IMG_WHT, 3, 1), \"xavier_normal\", batch_norm, nn.ReLU(ETA)) # 128 x 128\n",
    "        self.conv9 = conv(f[4], 3, 3, 1, calc_conv_pad(IMG_WHT, IMG_WHT, 3, 1), \"xavier_normal\", batch_norm, nn.ReLU(ETA)) # 128 x 128\n",
    "        \n",
    "    def forward(self, picture, landmarks_real, landmarks_wanted): #img = x\n",
    "        #Ecoder\n",
    "        x = torch.cat([picture, landmarks_real, landmarks_wanted], dim = 1)\n",
    "        c0 = self.conv0(x)\n",
    "        c1 = self.conv1(c0)\n",
    "\n",
    "        c2 = self.conv2(c1)\n",
    "\n",
    "        c3 = self.conv3(c2)\n",
    "\n",
    "        c4 = self.conv4(c3)\n",
    "        tmp = self.num_flat_features(c4)\n",
    "        f1 = c4.view(x.size()[0], tmp)\n",
    "        f1 = self.fc1(f1)\n",
    "        f1 = self.relu(f1)\n",
    "        f1 = f1.unsqueeze(0)\n",
    "        maxout = self.maxout(f1)[0]\n",
    "        \n",
    "        #Decoder\n",
    "        #1\n",
    "        \n",
    "        f2 = self.fc2(maxout)\n",
    "        rsh = f2.reshape((x.size()[0], 64, 10, 10))\n",
    "        \n",
    "        dc01 = self.dc0_1(rsh)\n",
    "        \n",
    "        dc02 = self.dc0_2(dc01)\n",
    "        dc03 = self.dc0_3(dc02)\n",
    "        \n",
    "        #2\n",
    "        dc1r = self.dc1(torch.cat((rsh, c4), dim=1))\n",
    "        dc2r = self.dc2(torch.cat((dc1r, c3), dim=1))\n",
    "        pic_div_2 = nn.MaxPool2d(2)(picture)\n",
    "        pic_div_4 = nn.MaxPool2d(2)(pic_div_2)\n",
    "        dc3r = self.dc3(torch.cat((dc2r, c2, pic_div_4, dc01), dim=1))\n",
    "        dc4r = self.dc4(torch.cat((dc3r, c1, pic_div_2, dc02), dim=1))\n",
    "        #3\n",
    "        \n",
    "        c5 = self.conv5(dc2r)\n",
    "        c6 = self.conv6(dc3r)\n",
    "    \n",
    "        c7 = self.conv7(torch.cat((dc4r, c0, picture, dc03), dim=1))\n",
    "\n",
    "        c8 = self.conv8(c7)\n",
    "   \n",
    "        c9 = self.conv9(c8)\n",
    "   \n",
    "        \n",
    "        return c5, c6, c9  #img_32, img_64, img_128\n",
    "        #return picture, nn.MaxPool2d(2)(picture), nn.MaxPool2d(2)(nn.MaxPool2d(2)(picture))\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "             Layer (type)                Input Shape         Param #\n",
      "=======================================================================\n",
      "                 Conv2d-1          [-1, 9, 160, 160]          28,288\n",
      "                   ReLU-2         [-1, 64, 160, 160]               0\n",
      "            BatchNorm2d-3         [-1, 64, 160, 160]             128\n",
      "          ResidualBlock-4         [-1, 64, 160, 160]               0\n",
      "                 Conv2d-5         [-1, 64, 160, 160]          36,928\n",
      "                   ReLU-6         [-1, 64, 160, 160]               0\n",
      "                   ReLU-7         [-1, 64, 160, 160]               0\n",
      "                 Conv2d-8         [-1, 64, 160, 160]          36,928\n",
      "                   ReLU-9         [-1, 64, 160, 160]               0\n",
      "                  ReLU-10         [-1, 64, 160, 160]               0\n",
      "                Conv2d-11         [-1, 64, 160, 160]         102,464\n",
      "                  ReLU-12           [-1, 64, 80, 80]               0\n",
      "           BatchNorm2d-13           [-1, 64, 80, 80]             128\n",
      "         ResidualBlock-14           [-1, 64, 80, 80]               0\n",
      "                Conv2d-15           [-1, 64, 80, 80]          36,928\n",
      "                  ReLU-16           [-1, 64, 80, 80]               0\n",
      "                  ReLU-17           [-1, 64, 80, 80]               0\n",
      "                Conv2d-18           [-1, 64, 80, 80]          36,928\n",
      "                  ReLU-19           [-1, 64, 80, 80]               0\n",
      "                  ReLU-20           [-1, 64, 80, 80]               0\n",
      "                Conv2d-21           [-1, 64, 80, 80]          73,856\n",
      "                  ReLU-22          [-1, 128, 40, 40]               0\n",
      "           BatchNorm2d-23          [-1, 128, 40, 40]             256\n",
      "         ResidualBlock-24          [-1, 128, 40, 40]               0\n",
      "                Conv2d-25          [-1, 128, 40, 40]         147,584\n",
      "                  ReLU-26          [-1, 128, 40, 40]               0\n",
      "                  ReLU-27          [-1, 128, 40, 40]               0\n",
      "                Conv2d-28          [-1, 128, 40, 40]         147,584\n",
      "                  ReLU-29          [-1, 128, 40, 40]               0\n",
      "                  ReLU-30          [-1, 128, 40, 40]               0\n",
      "                Conv2d-31          [-1, 128, 40, 40]         295,168\n",
      "                  ReLU-32          [-1, 256, 20, 20]               0\n",
      "           BatchNorm2d-33          [-1, 256, 20, 20]             512\n",
      "         ResidualBlock-34          [-1, 256, 20, 20]               0\n",
      "                Conv2d-35          [-1, 256, 20, 20]         590,080\n",
      "                  ReLU-36          [-1, 256, 20, 20]               0\n",
      "                  ReLU-37          [-1, 256, 20, 20]               0\n",
      "                Conv2d-38          [-1, 256, 20, 20]         590,080\n",
      "                  ReLU-39          [-1, 256, 20, 20]               0\n",
      "                  ReLU-40          [-1, 256, 20, 20]               0\n",
      "                Conv2d-41          [-1, 256, 20, 20]       1,180,160\n",
      "                  ReLU-42          [-1, 512, 10, 10]               0\n",
      "           BatchNorm2d-43          [-1, 512, 10, 10]           1,024\n",
      "         ResidualBlock-44          [-1, 512, 10, 10]               0\n",
      "                Conv2d-45          [-1, 512, 10, 10]       2,359,808\n",
      "                  ReLU-46          [-1, 512, 10, 10]               0\n",
      "                  ReLU-47          [-1, 512, 10, 10]               0\n",
      "                Conv2d-48          [-1, 512, 10, 10]       2,359,808\n",
      "                  ReLU-49          [-1, 512, 10, 10]               0\n",
      "                  ReLU-50          [-1, 512, 10, 10]               0\n",
      "                Linear-51                [-1, 51200]      26,214,912\n",
      "                  ReLU-52                  [-1, 512]               0\n",
      "             MaxPool1d-53               [-1, 1, 512]               0\n",
      "                Linear-54                  [-1, 256]       1,644,800\n",
      "       ConvTranspose2d-55           [-1, 64, 10, 10]          32,800\n",
      "                  ReLU-56           [-1, 32, 40, 40]               0\n",
      "           BatchNorm2d-57           [-1, 32, 40, 40]              64\n",
      "       ConvTranspose2d-58           [-1, 32, 40, 40]           2,064\n",
      "                  ReLU-59           [-1, 16, 80, 80]               0\n",
      "           BatchNorm2d-60           [-1, 16, 80, 80]              32\n",
      "       ConvTranspose2d-61           [-1, 16, 80, 80]             520\n",
      "                  ReLU-62          [-1, 8, 160, 160]               0\n",
      "           BatchNorm2d-63          [-1, 8, 160, 160]              16\n",
      "       ConvTranspose2d-64          [-1, 576, 10, 10]       1,180,160\n",
      "                  ReLU-65          [-1, 512, 20, 20]               0\n",
      "           BatchNorm2d-66          [-1, 512, 20, 20]           1,024\n",
      "         ResidualBlock-67          [-1, 512, 20, 20]               0\n",
      "                Conv2d-68          [-1, 512, 20, 20]       2,359,808\n",
      "                  ReLU-69          [-1, 512, 20, 20]               0\n",
      "                  ReLU-70          [-1, 512, 20, 20]               0\n",
      "                Conv2d-71          [-1, 512, 20, 20]       2,359,808\n",
      "                  ReLU-72          [-1, 512, 20, 20]               0\n",
      "                  ReLU-73          [-1, 512, 20, 20]               0\n",
      "         ResidualBlock-74          [-1, 512, 20, 20]               0\n",
      "                Conv2d-75          [-1, 512, 20, 20]       2,359,808\n",
      "                  ReLU-76          [-1, 512, 20, 20]               0\n",
      "                  ReLU-77          [-1, 512, 20, 20]               0\n",
      "                Conv2d-78          [-1, 512, 20, 20]       2,359,808\n",
      "                  ReLU-79          [-1, 512, 20, 20]               0\n",
      "                  ReLU-80          [-1, 512, 20, 20]               0\n",
      "       ConvTranspose2d-81          [-1, 768, 20, 20]         786,688\n",
      "                  ReLU-82          [-1, 256, 40, 40]               0\n",
      "           BatchNorm2d-83          [-1, 256, 40, 40]             512\n",
      "         ResidualBlock-84          [-1, 256, 40, 40]               0\n",
      "                Conv2d-85          [-1, 256, 40, 40]         590,080\n",
      "                  ReLU-86          [-1, 256, 40, 40]               0\n",
      "                  ReLU-87          [-1, 256, 40, 40]               0\n",
      "                Conv2d-88          [-1, 256, 40, 40]         590,080\n",
      "                  ReLU-89          [-1, 256, 40, 40]               0\n",
      "                  ReLU-90          [-1, 256, 40, 40]               0\n",
      "         ResidualBlock-91          [-1, 256, 40, 40]               0\n",
      "                Conv2d-92          [-1, 256, 40, 40]         590,080\n",
      "                  ReLU-93          [-1, 256, 40, 40]               0\n",
      "                  ReLU-94          [-1, 256, 40, 40]               0\n",
      "                Conv2d-95          [-1, 256, 40, 40]         590,080\n",
      "                  ReLU-96          [-1, 256, 40, 40]               0\n",
      "                  ReLU-97          [-1, 256, 40, 40]               0\n",
      "       ConvTranspose2d-98          [-1, 419, 40, 40]         214,656\n",
      "                  ReLU-99          [-1, 128, 80, 80]               0\n",
      "          BatchNorm2d-100          [-1, 128, 80, 80]             256\n",
      "        ResidualBlock-101          [-1, 128, 80, 80]               0\n",
      "               Conv2d-102          [-1, 128, 80, 80]         147,584\n",
      "                 ReLU-103          [-1, 128, 80, 80]               0\n",
      "                 ReLU-104          [-1, 128, 80, 80]               0\n",
      "               Conv2d-105          [-1, 128, 80, 80]         147,584\n",
      "                 ReLU-106          [-1, 128, 80, 80]               0\n",
      "                 ReLU-107          [-1, 128, 80, 80]               0\n",
      "        ResidualBlock-108          [-1, 128, 80, 80]               0\n",
      "               Conv2d-109          [-1, 128, 80, 80]         147,584\n",
      "                 ReLU-110          [-1, 128, 80, 80]               0\n",
      "                 ReLU-111          [-1, 128, 80, 80]               0\n",
      "               Conv2d-112          [-1, 128, 80, 80]         147,584\n",
      "                 ReLU-113          [-1, 128, 80, 80]               0\n",
      "                 ReLU-114          [-1, 128, 80, 80]               0\n",
      "      ConvTranspose2d-115          [-1, 211, 80, 80]          54,080\n",
      "                 ReLU-116         [-1, 64, 160, 160]               0\n",
      "          BatchNorm2d-117         [-1, 64, 160, 160]             128\n",
      "        ResidualBlock-118         [-1, 64, 160, 160]               0\n",
      "               Conv2d-119         [-1, 64, 160, 160]          36,928\n",
      "                 ReLU-120         [-1, 64, 160, 160]               0\n",
      "                 ReLU-121         [-1, 64, 160, 160]               0\n",
      "               Conv2d-122         [-1, 64, 160, 160]          36,928\n",
      "                 ReLU-123         [-1, 64, 160, 160]               0\n",
      "                 ReLU-124         [-1, 64, 160, 160]               0\n",
      "        ResidualBlock-125         [-1, 64, 160, 160]               0\n",
      "               Conv2d-126         [-1, 64, 160, 160]          36,928\n",
      "                 ReLU-127         [-1, 64, 160, 160]               0\n",
      "                 ReLU-128         [-1, 64, 160, 160]               0\n",
      "               Conv2d-129         [-1, 64, 160, 160]          36,928\n",
      "                 ReLU-130         [-1, 64, 160, 160]               0\n",
      "                 ReLU-131         [-1, 64, 160, 160]               0\n",
      "               Conv2d-132          [-1, 256, 40, 40]           6,915\n",
      "                 ReLU-133            [-1, 3, 40, 40]               0\n",
      "          BatchNorm2d-134            [-1, 3, 40, 40]               6\n",
      "               Conv2d-135          [-1, 128, 80, 80]           3,459\n",
      "                 ReLU-136            [-1, 3, 80, 80]               0\n",
      "          BatchNorm2d-137            [-1, 3, 80, 80]               6\n",
      "               Conv2d-138        [-1, 139, 160, 160]         222,464\n",
      "                 ReLU-139         [-1, 64, 160, 160]               0\n",
      "          BatchNorm2d-140         [-1, 64, 160, 160]             128\n",
      "               Conv2d-141         [-1, 64, 160, 160]          18,464\n",
      "                 ReLU-142         [-1, 32, 160, 160]               0\n",
      "          BatchNorm2d-143         [-1, 32, 160, 160]              64\n",
      "               Conv2d-144         [-1, 32, 160, 160]             867\n",
      "                 ReLU-145          [-1, 3, 160, 160]               0\n",
      "          BatchNorm2d-146          [-1, 3, 160, 160]               6\n",
      "=======================================================================\n",
      "Total params: 50,947,331\n",
      "Trainable params: 50,947,331\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(g, torch.zeros((1, 3, 160, 160)), torch.zeros((1, 3, 160, 160)), torch.zeros(1, 3, 160, 160))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disciminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        batch_norm = True\n",
    "        self.model = nn.Sequential(conv(6, 64, 4, 2, calc_conv_pad(IMG_WHT, IMG_WHT / 2, 4, 2), \"xavier_normal\", batch_norm, nn.ReLU(ETA)), # 64 x 64\n",
    "                                   conv(64, 128, 4, 2, calc_conv_pad(IMG_WHT / 2, IMG_WHT / 4, 4, 2), \"xavier_normal\", batch_norm, nn.ReLU(ETA)), # 32 x 32\n",
    "                                       conv(128, 256, 4, 2, calc_conv_pad(IMG_WHT / 4, IMG_WHT / 8, 4, 2), \"xavier_normal\", batch_norm, nn.ReLU(ETA)), # 16 x 16\n",
    "                                   conv(256, 512, 4, 2, calc_conv_pad(IMG_WHT / 8, IMG_WHT / 16, 4, 2), \"xavier_normal\", batch_norm, nn.ReLU(ETA)), # 8 x 8\n",
    "                                   conv(512, 512, 4, 1, calc_conv_pad(IMG_WHT / 16, 7, 4, 1), \"xavier_normal\", batch_norm, nn.ReLU(ETA)), # 7 x 7\n",
    "                                   conv(512, 1, 4, 1, calc_conv_pad(7, 6, 4, 1), \"xavier_normal\", batch_norm, nn.Sigmoid())) # 6 x 6\n",
    "    def forward(self, x, y):\n",
    "        x = torch.cat([x, y], dim = 1)\n",
    "        return nn.Softmax(dim=3)(self.model(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "          [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "          [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "          [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "          [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "          [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]]]],\n",
       "       grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d( torch.zeros((1, 3, 160, 160)),  torch.zeros((1, 3, 160, 160)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "multipie_path = r'multi_PIE_crop_128/001/001_01_01_051_00_crop_128.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### НЕ ЗАПУСКАТЬ, ЕСЛИ  папки img_list и frontal_faces УЖЕ ЕСТЬ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_DIR = 'multi_PIE_crop_128'\n",
    "subdirs = [x[0] for x in os.walk(MAIN_DIR)][1:]\n",
    "for subdir in subdirs:\n",
    "    for f in os.listdir(subdir):\n",
    "        if (os.path.isfile(os.path.join(subdir, f))):\n",
    "            tmp = f.split('.')[0].split('_')\n",
    "            path = '\\\\'.join([subdir, f])\n",
    "            if (tmp[3] == '051' and tmp[4] == '07' and tmp[6] == '128'):\n",
    "                with Image.open(path) as img:\n",
    "                    img.thumbnail((40, 40),Image.ANTIALIAS)\n",
    "                    path_40 = '\\\\'.join(['frontal_faces', f.split('.')[0][:-3] + '40.png'])\n",
    "                    img.save(path_40, 'PNG', quality=80)\n",
    "                with Image.open(path) as img:\n",
    "                    img.thumbnail((80, 80),Image.ANTIALIAS)\n",
    "                    path_80 = '\\\\'.join(['frontal_faces', f.split('.')[0][:-3] + '80.png'])\n",
    "                    img.save(path_80, 'PNG', quality=80)\n",
    "                with Image.open(path) as img:\n",
    "                    img = img.resize((160, 160),Image.ANTIALIAS)\n",
    "                    path_160 = '\\\\'.join(['frontal_faces', f.split('.')[0][:-3] + '160.png'])\n",
    "                    img.save(path_160, 'PNG', quality=80)\n",
    "            elif (f.split('.')[0].split('_')[4] == '07' and tmp[6] == '128'):\n",
    "                    with Image.open(path) as img:\n",
    "                        img = img.resize((160, 160),Image.ANTIALIAS)\n",
    "                        path_160 = '\\\\'.join(['img_list', f.split('.')[0][:-3] + '160.png'])\n",
    "                        img.save(path_160, 'PNG', quality=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from imutils import face_utils\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ПОДГОТОВИЛИ DLIB\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "LAND_MARKS_INDEXES = [37, 40, 43, 46, 34, 49, 67, 55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_LIST_PATH = 'img_list'\n",
    "FRONTAL_FACES_PATH = 'frontal_faces'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_landmarks(link):\n",
    "        img = cv2.imread(link)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        # The square where we place landmarks\n",
    "        black = cv2.imread(\"black_square160.png\")\n",
    "        rects = detector(gray, 0)\n",
    "        for rect in rects:\n",
    "            shape = predictor(gray, rect)\n",
    "            shape = face_utils.shape_to_np(shape)\n",
    "            for (i, (x, y)) in enumerate(shape):\n",
    "                if ((i + 1) in LAND_MARKS_INDEXES):\n",
    "                    cv2.circle(black, (x, y), 2, (0, 255, 0), -1)\n",
    "        return transforms.ToTensor()(Image.fromarray(black))\n",
    "\n",
    "\n",
    "class TrainDataset( Dataset):\n",
    "    def __init__( self , img_list ):\n",
    "        super(type(self),self).__init__()\n",
    "        self.img_list = img_list\n",
    "    def __len__( self ):\n",
    "        return len(self.img_list)\n",
    "    def __getitem__( self , idx ):\n",
    "        #example - 001_01_01_010_05_crop_128\n",
    "        batch = {}\n",
    "        img_name = self.img_list[idx]\n",
    "        img_frontal_name = '_'.join(img_name.split('_')[:3]) + '_051_07_crop_'\n",
    "        with Image.open('/'.join([IMG_LIST_PATH, img_name])) as i:\n",
    "            batch['img'] = transforms.ToTensor()(i)\n",
    "        with Image.open( '/'.join([FRONTAL_FACES_PATH, img_frontal_name + '160.png'])) as i:\n",
    "            batch['img160_wanted'] = transforms.ToTensor()(i)\n",
    "        with Image.open( '/'.join([FRONTAL_FACES_PATH, img_frontal_name + '40.png'])) as i:\n",
    "            batch['img40_wanted'] = transforms.ToTensor()(i)\n",
    "        with Image.open( '/'.join([FRONTAL_FACES_PATH, img_frontal_name + '80.png'])) as i:\n",
    "            batch['img80_wanted'] = transforms.ToTensor()(i)\n",
    "        # GET LANDMARKS\n",
    "\n",
    "        batch['landmarks_real'] = get_landmarks('/'.join([IMG_LIST_PATH, img_name]))\n",
    "        batch['landmarks_wanted'] = get_landmarks('/'.join([FRONTAL_FACES_PATH, img_frontal_name + '160.png']))\n",
    "        \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Составим img_list из фоток с поворотом не больше чем на +- 60 градусов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img_list = []\n",
    "for f in os.listdir(IMG_LIST_PATH):\n",
    "    tmp = f.split('.')[0].split('_')\n",
    "    angle = int(tmp[3])\n",
    "    if ((angle > 30 and angle <= 80) or (angle >= 130 and angle <= 180)):\n",
    "        img_list.append(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Shuffle and divide into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "val_split = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(0, len(img_list)))\n",
    "split = int(np.floor(val_split * len(img_list)))\n",
    "np.random.seed(random_seed)\n",
    "np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_list = list(np.array(img_list)[train_indices])\n",
    "test_img_list = list(np.array(img_list)[val_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = TrainDataset(train_img_list)\n",
    "test = TrainDataset(test_img_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader( train , batch_size = 4 , shuffle = True) \n",
    "test_dataloader = torch.utils.data.DataLoader( test , batch_size = 4 , shuffle = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extractor \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as ttf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_requires_grad(module , b ):\n",
    "    for parm in module.parameters():\n",
    "        parm.requires_grad = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model,dirname,epoch):\n",
    "    if type(model).__name__ == torch.nn.DataParallel.__name__:\n",
    "        model = model.module\n",
    "    torch.save( model.state_dict() , '{}/{}_epoch{}.pth'.format(dirname,type(model).__name__,epoch ) )\n",
    "    \n",
    "def save_optimizer(optimizer,model,dirname,epoch):\n",
    "    if type(model).__name__ ==  torch.nn.DataParallel.__name__:\n",
    "        model = model.module\n",
    "    torch.save( optimizer.state_dict() , '{}/{}_epoch_{}.pth'.format(dirname,type(optimizer).__name__ +'_' +type(model).__name__,epoch ) )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##EPOCH 0 ##\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "for param in resnet.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "G = nn.DataParallel(Generator()).to(device)\n",
    "D1 = nn.DataParallel(Discriminator()).to(device) # Этот дискриминатор отличает изображение (img) и то, как генератор его повернул \n",
    "D2 = nn.DataParallel(Discriminator()).to(device) # Этот дискриминатор отличает повернутое G изображение и landmarks_wanted \n",
    "\n",
    "optimizer_G = torch.optim.Adam(G.parameters(), lr = 1e-4)\n",
    "optimizer_D1 = torch.optim.Adam(D1.parameters(), lr = 1e-4) \n",
    "optimizer_D2 = torch.optim.Adam(D2.parameters(), lr = 1e-4) \n",
    "\n",
    "L1 = torch.nn.L1Loss().to(device)\n",
    "\n",
    "mse = torch.nn.MSELoss().to(device)\n",
    "\n",
    "cross_entropy = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "tb = SummaryWriter()  \n",
    "epoch = 0\n",
    "while(True):\n",
    "    print(\"##EPOCH\", epoch,\"##\")\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        for k in  batch:\n",
    "            batch[k] =  torch.autograd.Variable( batch[k].to(device) , requires_grad = False) \n",
    "        # adversarial loss\n",
    "        if (step % 100 == 0):\n",
    "            print(step)\n",
    "            \n",
    "        img_40_fake, img_80_fake, img_160_fake = G(batch['img'], batch['landmarks_real'], batch['landmarks_wanted'])\n",
    "        \n",
    "        set_requires_grad( D1 , True )\n",
    "        L_D1 = 0.1 *(torch.mean( - torch.log( D1(batch['img160_wanted'], batch['img'])) - torch.log( 1 -  D1(img_160_fake.detach(), batch['img']))))\n",
    "        optimizer_D1.zero_grad()\n",
    "        L_D1.backward()\n",
    "        optimizer_D1.step()\n",
    "        set_requires_grad( D1 , False )\n",
    "        \n",
    "        set_requires_grad( D2 , True )\n",
    "        L_D2 = 0.1*(torch.mean( - torch.log( D2(batch['img160_wanted'], batch['landmarks_wanted'])) - torch.log( 1 -  D2(img_160_fake.detach(), batch['landmarks_wanted']))))\n",
    "        optimizer_D2.zero_grad()\n",
    "        L_D2.backward()\n",
    "        optimizer_D2.step()\n",
    "        set_requires_grad( D2 , False )\n",
    "        \n",
    "        pixelwise_160_loss = L1(img_160_fake, batch['img160_wanted'])\n",
    "        pixelwise_80_loss = L1(img_80_fake, batch['img80_wanted'])\n",
    "        pixelwise_40_loss = L1(img_40_fake, batch['img40_wanted'])\n",
    "        features_real = resnet(batch['img160_wanted'])\n",
    "        features_fake = resnet(img_160_fake)\n",
    "\n",
    "        identity_loss = mse(features_real.detach(), features_fake)\n",
    "        \n",
    "        total_variation = torch.mean( torch.abs(  img_160_fake[:,:,:-1,:] - img_160_fake[:,:,1:,:] ) )  + torch.mean(  torch.abs( img_160_fake[:,:,:,:-1] - img_160_fake[:,:,:,1:] ) )  \n",
    "        \n",
    "        L_final = 10 * (pixelwise_160_loss + pixelwise_80_loss + pixelwise_40_loss) + 0.02 * identity_loss + (1e-4) *  total_variation\n",
    "        optimizer_G.zero_grad()\n",
    "        L_final.backward()\n",
    "        optimizer_G.step()\n",
    "      \n",
    "        tb.add_scalar( \"D1_loss/Train\" , L_D1.data.cpu().numpy() ,  epoch*len(train_dataloader) + step)\n",
    "        tb.add_scalar( \"D2_loss/Train\" , L_D2.data.cpu().numpy() ,  epoch*len(train_dataloader) + step)\n",
    "        tb.add_scalar( \"pixelwise_160_los/Train\" , pixelwise_160_loss.data.cpu().numpy() ,   epoch*len(train_dataloader) + step)\n",
    "        tb.add_scalar( \"identity_loss/Train\" , identity_loss.data.cpu().numpy() ,  epoch*len(train_dataloader) + step)\n",
    "        tb.add_scalar( \"total_variation_loss/Train\" , total_variation.data.cpu().numpy() ,  epoch*len(train_dataloader) + step)\n",
    "        tb.add_scalar( \"final_loss/Train\" , L_final.data.cpu().numpy() , epoch*len(train_dataloader) + step )\n",
    "   \n",
    "            \n",
    "    \n",
    "    save_model(G, \"models_save\", epoch)\n",
    "    save_model(D1, \"models_save\", epoch)\n",
    "    save_model(D2, \"models_save\", epoch)\n",
    "    save_optimizer(optimizer_G,G, \"optimizer_save\",epoch)\n",
    "    save_optimizer(optimizer_D1,D1, \"optimizer_save\",epoch)\n",
    "    save_optimizer(optimizer_D2,D2, \"optimizer_save\",epoch)\n",
    "    \n",
    "    print(\"VALIDATION\")\n",
    "    img_batch = np.zeros((4, 3, 160, 160))\n",
    "    img_batch_index = 0\n",
    "    identity_mse = 0\n",
    "    for step, batch in enumerate(test_dataloader):\n",
    "        for k in  batch:\n",
    "            batch[k] =  torch.autograd.Variable( batch[k].to(device) , requires_grad = False)\n",
    "            \n",
    "        img_40_fake, img_80_fake, img_160_fake = G(batch['img'], batch['landmarks_real'], batch['landmarks_wanted'])\n",
    "        features_real = resnet(batch['img160_wanted'])\n",
    "        features_fake = resnet(img_160_fake)\n",
    "\n",
    "        identity_mse += mse(features_real.detach(), features_fake.clone().detach())\n",
    "        \n",
    "        if step % 30 == 0 :\n",
    "            print(step)\n",
    "            img_batch[img_batch_index % 4] = img_160_fake[0].cpu().clone().detach()\n",
    "            img_batch_index += 1\n",
    "    tb.add_scalar(\"identity_mse/Test\" , identity_mse.data.cpu().numpy() / len(test_dataloader),  epoch)\n",
    "    tb.add_images(\"img_160_fake/Validation\", img_batch, epoch)\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
